{
    "content": "A Tour of Machine Learning Algorithms\nby Jason Brownlee on October 11, 2023 in Machine Learning Algorithms 359\n Tweet Share\nIn this post, we will take a tour of the most popular machine learning algorithms.\n\nIt is useful to tour the main algorithms in the field to get a feeling of what methods are available.\n\nThere are so many algorithms that it can feel overwhelming when algorithm names are thrown around and you are expected to just know what they are and where they fit.\n\nI want to give you two ways to think about and categorize the algorithms you may come across in the field.\n\nThe first is a grouping of algorithms by their learning style.\nThe second is a grouping of algorithms by their similarity in form or function (like grouping similar animals together).\nBoth approaches are useful, but we will focus in on the grouping of algorithms by similarity and go on a tour of a variety of different algorithm types.\n\nAfter reading this post, you will have a much better understanding of the most popular machine learning algorithms for supervised learning and how they are related.\n\nKick-start your project with my new book Master Machine Learning Algorithms, including step-by-step tutorials and the Excel Spreadsheet files for all examples.\n\nLet\u2019s get started.\n\nEnsemble Learning Method\nA cool example of an ensemble of lines of best fit. Weak members are grey, the combined prediction is red.\nPlot from Wikipedia, licensed under public domain.\n\nAlgorithms Grouped by Learning Style\nThere are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data.\n\nIt is popular in machine learning and artificial intelligence textbooks to first consider the learning styles that an algorithm can adopt.\n\nThere are only a few main learning styles or learning models that an algorithm can have and we\u2019ll go through them here with a few examples of algorithms and problem types that they suit.\n\nThis taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.\n\n\nLet\u2019s take a look at three different learning styles in machine learning algorithms:\n1. Supervised Learning\nSupervised Learning AlgorithmsInput data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.\n\nA model is prepared through a training process in which it is required to make predictions and is corrected when those predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.\n\nExample problems are classification and regression.\n\nExample algorithms include: Logistic Regression and the Back Propagation Neural Network.\n\n\n2. Unsupervised Learning\nUnsupervised Learning AlgorithmsInput data is not labeled and does not have a known result.\n\nA model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redundancy, or it may be to organize data by similarity.\n\nExample problems are clustering, dimensionality reduction and association rule learning.\n\nExample algorithms include: the Apriori algorithm and K-Means.\n\n3. Semi-Supervised Learning\nSemi-supervised Learning AlgorithmsInput data is a mixture of labeled and unlabelled examples.\n\nThere is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.\n\nExample problems are classification and regression.\n\nExample algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data.\n\n\nOverview of Machine Learning Algorithms\nWhen crunching data to model business decisions, you are most typically using supervised and unsupervised learning methods.\n\nA hot topic at the moment is semi-supervised learning methods in areas such as image classification where there are large datasets with very few labeled examples.\n\nAlgorithms Grouped By Similarity\nAlgorithms are often grouped by similarity in terms of their function (how they work). For example, tree-based methods, and neural network inspired methods.\n\nI think this is the most useful way to group algorithms and it is the approach we will use here.\n\nThis is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describe the problem and the class of algorithm such as Regression and Clustering.\n\nWe could handle these cases by listing algorithms twice or by selecting the group that subjectively is the \u201cbest\u201d fit. I like this latter approach of not duplicating algorithms to keep things simple.\n\nIn this section, we list many of the popular machine learning algorithms grouped the way we think is the most intuitive. The list is not exhaustive in either the groups or the algorithms, but I think it is representative and will be useful to you to get an idea of the lay of the land.\n\nPlease Note: There is a strong bias towards algorithms used for classification and regression, the two most prevalent supervised machine learning problems you will encounter.\n\nIf you know of an algorithm or a group of algorithms not listed, put it in the comments and share it with us. Let\u2019s dive in.\n\n\nRegression Algorithms\nRegression AlgorithmsRegression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.\n\nRegression methods are a workhorse of statistics and have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.\n\nThe most popular regression algorithms are:\n\nOrdinary Least Squares Regression (OLSR)\nLinear Regression\nLogistic Regression\nStepwise Regression\nMultivariate Adaptive Regression Splines (MARS)\nLocally Estimated Scatterplot Smoothing (LOESS)\n\nInstance-based Algorithms\nInstance-based AlgorithmsInstance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.\n\nSuch methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.\n\nThe most popular instance-based algorithms are:\n\nk-Nearest Neighbor (kNN)\nLearning Vector Quantization (LVQ)\nSelf-Organizing Map (SOM)\nLocally Weighted Learning (LWL)\nSupport Vector Machines (SVM)\nRegularization Algorithms\nRegularization AlgorithmsAn extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.\n\nI have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.\n\nThe most popular regularization algorithms are:\n\nRidge Regression\nLeast Absolute Shrinkage and Selection Operator (LASSO)\nElastic Net\nLeast-Angle Regression (LARS)\nDecision Tree Algorithms\nDecision Tree AlgorithmsDecision tree methods construct a model of decisions made based on actual values of attributes in the data.\n\nDecisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.\n\nThe most popular decision tree algorithms are:\n\nClassification and Regression Tree (CART)\nIterative Dichotomiser 3 (ID3)\nC4.5 and C5.0 (different versions of a powerful approach)\nChi-squared Automatic Interaction Detection (CHAID)\nDecision Stump\nM5\nConditional Decision Trees\n\nBayesian Algorithms\nBayesian AlgorithmsBayesian methods are those that explicitly apply Bayes\u2019 Theorem for problems such as classification and regression.\n\nThe most popular Bayesian algorithms are:\n\nNaive Bayes\nGaussian Naive Bayes\nMultinomial Naive Bayes\nAveraged One-Dependence Estimators (AODE)\nBayesian Belief Network (BBN)\nBayesian Network (BN)\nClustering Algorithms\nClustering AlgorithmsClustering, like regression, describes the class of problem and the class of methods.\n\nClustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.\n\nThe most popular clustering algorithms are:\n\nk-Means\nk-Medians\nExpectation Maximisation (EM)\nHierarchical Clustering\n\nAssociation Rule Learning Algorithms\nAssoication Rule Learning AlgorithmsAssociation rule learning methods extract rules that best explain observed relationships between variables in data.\n\nThese rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.\n\nThe most popular association rule learning algorithms are:\n\nApriori algorithm\nEclat algorithm\nArtificial Neural Network Algorithms\nArtificial Neural Network AlgorithmsArtificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks.\n\nThey are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.\n\nNote that I have separated out Deep Learning from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.\n\nThe most popular artificial neural network algorithms are:\n\nPerceptron\nMultilayer Perceptrons (MLP)\nBack-Propagation\nStochastic Gradient Descent\nHopfield Network\nRadial Basis Function Network (RBFN)\n\nDeep Learning Algorithms\nDeep Learning AlgorithmsDeep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation.\n\nThey are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with very large datasets of labelled analog data, such as image, text. audio, and video.\n\nThe most popular deep learning algorithms are:\n\nConvolutional Neural Network (CNN)\nRecurrent Neural Networks (RNNs)\nLong Short-Term Memory Networks (LSTMs)\nStacked Auto-Encoders\nDeep Boltzmann Machine (DBM)\nDeep Belief Networks (DBN)\nDimensionality Reduction Algorithms\nDimensional Reduction AlgorithmsLike clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarize or describe data using less information.\n\nThis can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.\n\nPrincipal Component Analysis (PCA)\nPrincipal Component Regression (PCR)\nPartial Least Squares Regression (PLSR)\nSammon Mapping\nMultidimensional Scaling (MDS)\nProjection Pursuit\nLinear Discriminant Analysis (LDA)\nMixture Discriminant Analysis (MDA)\nQuadratic Discriminant Analysis (QDA)\nFlexible Discriminant Analysis (FDA)\nt-distributed Stochastic Neighbor Embedding (t-SNE)\nUniform Manifold Approximation and Projection for Dimension Reduction (UMAP)\nEnsemble Algorithms\nEnsemble AlgorithmsEnsemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.\n\nMuch effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.\n\nBoosting\nBootstrapped Aggregation (Bagging)\nAdaBoost\nWeighted Average (Blending)\nStacked Generalization (Stacking)\nGradient Boosting Machines (GBM)\nGradient Boosted Regression Trees (GBRT)\nRandom Forest\n\nOther Machine Learning Algorithms\nMany algorithms were not covered.\n\nI did not cover algorithms from specialty tasks in the process of machine learning, such as:\n\nFeature selection algorithms\nAlgorithm accuracy evaluation\nPerformance measures\nOptimization algorithms\nI also did not cover algorithms from specialty subfields of machine learning, such as:\n\nComputational intelligence (evolutionary algorithms, etc.)\nComputer Vision (CV)\nNatural Language Processing (NLP)\nRecommender Systems\nReinforcement Learning\nGraphical Models\nAnd more\u2026\nThese may feature in future posts.\n\nFurther Reading on Machine Learning Algorithms\nThis tour of machine learning algorithms was intended to give you an overview of what is out there and some ideas on how to relate algorithms to each other.\n\nI\u2019ve collected together some resources for you to continue your reading on algorithms. If you have a specific question, please leave a comment.\n\n\nOther Lists of Machine Learning Algorithms\nThere are other great lists of algorithms out there if you\u2019re interested. Below are few hand selected examples.\n\nList of Machine Learning Algorithms: On Wikipedia. Although extensive, I do not find this list or the organization of the algorithms particularly useful.\nMachine Learning Algorithms Category: Also on Wikipedia, slightly more useful than Wikipedias great list above. It organizes algorithms alphabetically.\nCRAN Task View: Machine Learning & Statistical Learning: A list of all the packages and all the algorithms supported by each machine learning package in R. Gives you a grounded feeling of what\u2019s out there and what people are using for analysis day-to-day.\nTop 10 Algorithms in Data Mining: on the most popular algorithms for data mining. Another grounded and less overwhelming take on methods that you could go off and learn deeply.\nHow to Study Machine Learning Algorithms\nAlgorithms are a big part of machine learning. It\u2019s a topic I am passionate about and write about a lot on this blog. Below are few hand selected posts that might interest you for further reading.\n\nHow to Learn Any Machine Learning Algorithm: A systematic approach that you can use to study and understand any machine learning algorithm using \u201calgorithm description templates\u201d (I used this approach to write my first book).\nHow to Create Targeted Lists of Machine Learning Algorithms: How you can create your own systematic lists of machine learning algorithms to jump start work on your next machine learning problem.\nHow to Research a Machine Learning Algorithm: A systematic approach that you can use to research machine learning algorithms (works great in collaboration with the template approach listed above).\nHow to Investigate Machine Learning Algorithm Behavior: A methodology you can use to understand how machine learning algorithms work by creating and executing very small studies into their behavior. Research is not just for academics!\nHow to Implement a Machine Learning Algorithm: A process and tips and tricks for implementing machine learning algorithms from scratch.\n\nHow to Run Machine Learning Algorithms\nSometimes you just want to dive into code. Below are some links you can use to run machine learning algorithms, code them up using standard libraries or implement them from scratch.\n\nHow To Get Started With Machine Learning Algorithms in R: Links to a large number of code examples on this site demonstrating machine learning algorithms in R.\nMachine Learning Algorithm Recipes in scikit-learn: A collection of Python code examples demonstrating how to create predictive models using scikit-learn.\nHow to Run Your First Classifier in Weka: A tutorial for running your very first classifier in Weka (no code required!).\nFinal Word\nI hope you have found this tour useful.\n\nPlease, leave a comment if you have any questions or ideas on how to improve the algorithm tour.\n\nUpdate: Continue the discussion on HackerNews and reddit."
}